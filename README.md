# AI-Research-Paper-List
List of all the AI Research Papers I have found and used

## Computer Vision (CV)

## Natural Language Processing (NLP)

## LLMs

### Miscellaneous
* [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf) 
* [Levels of AGI: Operationalizing Progress on the Path to AGI](https://arxiv.org/pdf/2311.02462)

### Architecture/Models
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) 

### Prompting/Agents
* [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682) 
* [The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”](https://arxiv.org/pdf/2309.12288.pdf) 
* [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172) 
* [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165) 
* [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837) 
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903) 
* [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2201.11903) 
* [“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/pdf/2308.03825.pdf) 
* [Many-shot Jailbreaking](https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf) 
* [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/pdf/2309.04269.pdf) 
* [PromptChainer: Chaining Large Language Model Prompts through Visual Programming](https://arxiv.org/pdf/2203.06566) 
* [Large Language Models Understand and Can Be Enhanced by Emotional Stimuli](https://arxiv.org/pdf/2307.11760) 
* [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171) 
* [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629) 
* [CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments](https://arxiv.org/pdf/2404.18021) 
* [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601) 
* [Large Language Model Guided Tree-of-Thought](https://arxiv.org/pdf/2305.08291) 
* [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300) 
* [MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark](https://arxiv.org/abs/2406.01574) 
* [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)
* [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388)





